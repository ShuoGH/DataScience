{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import axes3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularised Linear Regression with 1-norm and 2-norm weight penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook helps you see the effect of introducing weight penalties for learning tasks. While the learning task dealt with here is linear regression for illustrative purposes, the core ideas will generalise for other tasks as well. We revise the material on gradient descent before we progress to the main objective of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the gradient is:\n",
    "(∇wL)i=∂L∂wi=−2∑k=1NAki(yk−∑j=1pAkjwj)=−2(AT(y−Aw))i.\n",
    "\n",
    "(this formula shoule be reviewed later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent(Amat,y,winit,rate,numiter):\n",
    "    n,p=Amat.shape\n",
    "    whistory=[]\n",
    "    losshistory=[]\n",
    "    w=winit\n",
    "    for i in range(numiter):\n",
    "        loss=np.square(y-Amat.dot(w)).mean()\n",
    "        whistory.append(w)\n",
    "        losshistory.append(loss)\n",
    "        grad=(-2/n)*Amat.T.dot((y-Amat.dot(w)))\n",
    "        w=w-rate*grad\n",
    "    return w,np.asarray(whistory),np.asarray(losshistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgdescent(Amat,y,winit,tn,td,numepochs):\n",
    "    n,p=Amat.shape\n",
    "    whistory=[]\n",
    "    losshistory=[]\n",
    "    w=winit\n",
    "    for epoch in range(numepochs):\n",
    "#        rss=np.square(y-Amat.dot(w)).mean()  \n",
    "# it seems that it's wrong. rss should be LOSS\n",
    "        loss=np.square(y-Amat.dot(w)).mean()\n",
    "        whistory.append(w)\n",
    "        losshistory.append(loss)\n",
    "        for k in range(n):      #n equals to the \n",
    "            idx=np.random.randint(n)\n",
    "            xi=Amat[idex:idx+1]\n",
    "            yi=y[idx:idx+1]\n",
    "            grad=(-2)*xi.T.dot((yi-xi.dot(W)))\n",
    "            rate=tn/(td+epoch*n+k)\n",
    "            \n",
    "            w=w-rate*grad\n",
    "        return w,np.asarray(whistory),np.asarray(losshistory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
